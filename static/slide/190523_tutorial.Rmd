---
title: "GNU MCSim Tutorial 3"
subtitle: "Markov Chain Monte Carlo Calibration<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>"
author: "Nan-Hung Hsieh"
date: "May 23, 2019"
output:
  xaringan::moon_reader:
    css: ["libs/remark-css-0.0.1/default.css", "libs/remark-css-0.0.1/metropolis.css", "libs/remark-css-0.0.1/metropolis-fonts.css"]
    nature:
      beforeInit: "macros.js"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_knit$set(root.dir =  "C:/MinGW/msys/1.0/home/nhsieh/MCSim_under_R")
#knitr::opts_knit$set(root.dir =  "/Users/nanhung/Rproj/MCSim_under_R")
knitr::opts_knit$set(root.dir =  "/home/nanhung/MCSim_under_R")
```

```{r, include=FALSE}
source("MCSim/function.R")
```


# Outline

## 1. Markov Chain Monte Carlo

- Principle & Workflow

- **Example: linear model**

## 2. `MCMC()`

- Working with GNU MCSim & R

- **Example: Ethylbenzene PBPK model**

## 3. Population modeling

- Single and multi level

- **Example: Tetrachloroethylene PBPK model**

???

In this tutorial we will focus MCMC calibration. Which means we want to calibrate the model parameters and find the likelihood of model parameter to fit the data.

I will start from introducing the MCMC principle. What is Bayesian? What is MCMC? How do we apply this technique in our modeling process. Also I will summarie the overall workflow include the contain in our previous tutorials. And, I will use the linear model as an example to explain the basic knowledge in MCMC calibration.

The second section is to exalpain how to use mCSim to Run MCMC simulation and use R to analyze and visualzie the model output. We will go through the Ethylbenzene PBPK model as an example to demo the full workflow from model verification, uncertainty and sensitivity analysis and finally demostrate the MCMC calibration in this case.

After the ethylbenzene PBPK model, I'll use the tetrachloroethylene PBPK model to explain how to apply hierarchical approach to calibrate the parameter in population and individual levels.

---

# General workflow

### 1 Model constructing or translating

### 2 Verify modeling result

- **Compare with published result**
- **Mass balance** 

### 3 Uncertainty and sensitivity analysis

- **Monte Carlo simulation**
- **Morris elementary effects screening** 
- **Fourier amplitude sensitivity test**

### 4 Model calibration 

- **Markov chain Monte Carlo** 
  - Diagnostics (Goodness-of-fit, convergence)

???

Here is the review of the general workflow in our modeling process. In the first tutorial, I explain the basic knowledge in PK modeling. We can translate the model code from other programming language to MCSim and conduct the straight forward simulation. We had some exercise to run the model and verify the modeling result through comparing the published data. 

Then, in the uncertainty and sensitivity analysis, we can apply Monte Carlo simulation with the given distribution of model parameter to exam whether the experiment data can be covered in uncertainty or the distribution of model output.
If our data can not cover in the range of our simulation, we might need to figure out which parameters had significant impact on the model output. 
So through Morris elementary effects screening we can find the influential parameter for specific varibale in our model than we can adjust the parameter range to have better prediction. 
In addtion, if we have too many parameters that might cause the computational burden in MCMC calibration, we might need to apply the global sensitivity approach to detect the noninfluential parameter and fix this parameter at baseline value.

Finally, we can us MCMC algorithm to do model calibration. This is the today's topic as well.

---

# MC & MCMC

## Monte Carlo

### - Generate the probability distribution based on the given condition

</br>

## Markov Chain Monte Carlo

### - Iterative update the probability distribution, if the new proposed distribution is accepted. 

---

# Bayes' rule

.font200[
$$ p(\theta|y) = \frac{p(\theta) p(y|\theta) }{p(y)}$$
]

$\theta$: **Observed or unobserved parameter**

$y$: **Observed data**

</br>

$p(\theta)$: *Prior distribution* of model parameter

$p(y|\theta)$: *Sampling distribution* of the experiment data given by a parameter vector

$p(y)$: *Likelihood* of data

$p(\theta|y)$: *Posterior distribution*

???

The MCMC ias based on the Bayesian rule. In this function, it includes the density of observed or unobserved parameter that are often referred to as the prior distribution in our model, which is p(theta). 

Also, we have the sampling distribution (or data distribution), which is p(y|theta). This term is used to explain the probability of observed result that is given by the model parameter

Then, we need to have the likelihood to describe the probability distribution of the experiment data (y). And through this information the posterior can be estimated by this equation.

---

# Monty Hall Problem

> Suppose you’re on a game show, and you’re given the choice of three doors. Behind one door is a car, behind the others, goats. You pick a door, say number 1, and the host, who knows what’s behind the doors, opens another door, say number 3, which has a goat. He says to you, ”Do you want to pick door number 2?” Is it to your advantage to switch your choice of doors?

> Marilyn vos Savant. 1990. Ask Marilyn. *Parade Magazine*: 16.

.pull-left[
![](https://upload.wikimedia.org/wikipedia/commons/d/de/Monty_tree_door1.svg)
]

.pull-right[

```{R echo=F, fig.height=7, fig.width=9}
N <- 500
set.seed(7)
prize <- sample(1:3, N, replace = TRUE)
stay <- sample(1:3, N, replace = TRUE)
reveal <- rep(0, N)
change <- rep(0, N)
for(i in 1:N) {
  x <- c(1:3)[-c(prize[i], stay[i])]
  reveal[i] <- x[sample.int(length(x), size = 1)]
  change[i] <- c(1:3)[-c(reveal[i], stay[i])]
}
changewin <- ifelse(change == prize, 1, 0)
staywin <- ifelse(stay == prize, 1, 0)
change_perc <- mean(changewin)
stay_perc <- mean(staywin)
plot(cumsum(changewin) / c(1:N), main = "'Convergence' to True Winning Proportions",
     xlab = "Trial", ylab = "Win Percent", ylim = c(0, 1), col = "blue")
abline(h = 2/3)
points(cumsum(staywin) / c(1:N), type = "p", col = "red")
abline(h = 1/3)
legend("topright", legend = c("Switch to Other Door","Don't Switch"), pch = 1 , col = c(4,2))
```

]

???

The famous example of bayesian rule in the real life is Monty Hall problem. Suppose you’re on a game show, you have a chance to win a car that is behind the one of three door, so the probability of winning the car is one-third. But the host give the contestent the information, he open the door that is a goat behind this door and ask contest to swith from the chosen door or not. 
At this time in this case, the question is change to what is the probability to win the car if the contestent switch the choice. It's not the 50/50 probability. 

The diagram here explain the conditional probability of winning by switching the door. If contestent switch the door, it will has 67% probability to win the game compare with 33% if staying with the original choice. 

---

# Markov Chain Monte Carlo

.font120[
- **Metropolis-Hastings sampling algorithm**
]

The algorithm was named for Nicholas Metropolis (physicist) and Wilfred Keith Hastings (statistician). The algorithm proceeds as follows.

**Initialize**

1. Pick an initial parameter sets $\theta_{t=0} = \{\theta_1, \theta_2, ... \theta_n\}$

**Iterate**

1. *Generate*: randomly generate a candidate parameter state $\theta^\prime$ for the next sample by picking from the conditional distribution  $J(\theta^\prime|\theta_t)$
2. *Compute*:  compute the acceptance probability 
$A\left(\theta^{\prime}, \theta_{t}\right)=\min \left(1, \frac{P\left(\theta^{\prime}|y\right)}{P\left(\theta_{t}|y\right)} \frac{J\left(\theta_{t} | \theta^{\prime}\right)}{J\left(\theta^{\prime} | \theta_{t}\right)}\right)$
2. *Accept or Reject*:
  1. generate a uniform random number $u \in[0,1]$
  2. if $u \leq A\left(x^{\prime}, x_{t}\right)$ accept the new state and set $\theta_{t+1}=\theta^{\prime}$, otherwise reject the new state, and copy the old state forward $\theta_{t+1}=\theta_{t}$

???

Markov chain Monte Carlo is a general method based on drawing values of parameter theta from approximate distributions and then correcting those draws to better approximate target posterior p(theta|y). The Metrapolis-Hastings algorithm is a general term for a family of Markov chain simulation methods that are useful for drawing samples from Bayesian posterior distribution. The Metrpolis algorithm is an adaption of a random walk that uses an acceptance/rejection rule to the target distribution. 

In the beginning the it draw a strating point for the initial parameter sets based on the given prior parameter setting. Then, sample the candidate parameter state from a jumping distribution, which is J of theta prime given theta t. 
The second step is to compute the acceptance probability from the ratio of posterior and jumpling distribution.
Finally, uses the acceptance probability and compare with the random number that generate from uniform distribution between 0 and 1 to determine the proposed parameter vector can be keeped or discard in the next iteration.

show animation

---

class: middle

.font200[
## The product of output is not ~~best-fit~~, but "prior" and "posterior".
]

???

In the MCMC process, the pupose is not to estimate the maximun likelihood, but it attemp to continuous construct the probability of the parameter estimation. The posterior in the current iteration will become the prior of next iteration.

---

# log-likelihood

The log-likelihood function was used to assess the **goodness-of-fit** of the model to the data ([Woodruff and Bois, 1993](https://www.sciencedirect.com/science/article/pii/0378427493901035?via%3Dihub), [Hsieh et al., 2018](https://www.frontiersin.org/articles/10.3389/fphar.2018.00588/full#B43))

$$L L=\sum_{i=1}^{N}-\frac{1}{2} \cdot \frac{\left(y_{i}-\widehat{y}_{i}\right)^{2}}{S_{j[i]}^{2}}-\frac{1}{2} \ln \left(2 \pi s_{j[i]}^{2}\right)$$

$N$: the total number of the data points 

$y_i$: experimental observed

$\hat{y}_i$: model predicted value

$j[i]$: data type (output variable) of data point $i$

$S_{j[i]}^{2}$: the variance for data type $j$

???

There is a log-likelihood function that can be used to estimate the goodness-of-fit. This function used the information include ... to estimate log-likelihood. In our study, we used this function to estimate the model performance when we apply parameter fixing. The log-likelihood of data can also estimate in MCSim in each iteration0 

---

# Calibration & evaluation

###  Prepare model and input files 
  - Need at least 4 chains in simulation

### Check convergence & graph the output result
  - **Parameter**, **log-likelihood of data**
  - Trace plot, density plot, correlation matrix, auto-correlation, running mean, ...
  - Gelman–Rubin convergence diagnostics

### Evaluate the model fit
  - Global evaluation
  - Individual evaluation 

---

# Example: linear modeling

.code60[

.pull-left[

**model-file**

```r
## linear.model.R ####

Outputs = {y}

# Model Parameters
A = 0; # Default value of intercept
B = 1; # Default value of slope

# Statistical parameter
SD_true = 0;

CalcOutputs { 
  y = A + B * t + NormalRandom(0,SD_true); 
}
End.

```
]

.pull-right[

**input-file**

```r
## ./mcsim.linear.model.R.exe linear_mcmc.in.R ####

MCMC ("MCMC.default.out","", # name of output file
     "",         # name of data file
     2000,0,     # iterations, print predictions flag
     1,2000,     # printing frequency, iters to print
     10101010);  # random seed (default)

Level {
  
  Distrib(A, Normal, 0, 2); # prior of intercept 
  Distrib(B, Normal, 1, 2); # prior of slope 
  
  Likelihood(y, Normal, Prediction(y), 0.5);
  
  Simulation {
    PrintStep (y, 0, 10, 1); 
    Data (y, 0.0, 0.15, 2.32, 4.33, 4.61, 6.68, 7.89, 7.13, 7.27, 9.4, 10.0);
  }
}
End.
  
```

]
]

---

# The data

```{r fig.height=4.6, dev='svg'}
x <- seq(0, 10, 1)
y <- c(0.0, 0.15, 2.32, 4.33, 4.61, 6.68, 7.89, 7.13, 7.27, 9.4, 10.0)
plot(x, y)
```

---

# MCMC sampling process

.font80[
```{r, echo=F, message=F}
model <- "linear.model.R"
input <- "linear_mcmc.in.R"
set.seed(1111) 
out1 <- mcsim(model, input) 
head(out1)
```
]

.pull-left[

**Trace plot**

```{r, echo=F, fig.height=6, message=F}
plot(out1$A.1., type = "l", xlab = "Iteration", ylab = "")
lines(out1$B.1., col = 2)
legend("topright", legend = c("Intercept", "Slope"), col = c(1,2), lty = 1)
```

]

.pull-right[

**Parameter space**

```{r, echo=F, fig.height=6, message=F}
plot(out1$A.1., out1$B.1., type = "b", xlab = "Intercept", ylab = "Slope")
```

]

---

# Trace plot

```{r, echo=F, message=F}
# Check convergence
library(bayesplot)
library(rstan)
model <- "linear.model.R"
input <- "linear_mcmc.in.R"
set.seed(2234); out2 <- mcsim(model, input) # Generate the 2nd chain
set.seed(3234); out3 <- mcsim(model, input) # Generate the 3rd chain
set.seed(4234); out4 <- mcsim(model, input) # Generate the 4th chain
sims <- mcmc_array(data = list(out1,out2,out3,out4))
color_scheme_set("mix-blue-red")
```

```{r fig.height=5, fig.width=13, dev='svg'}
parms_name <- c("A.1.","B.1.")
mcmc_trace(sims, pars = parms_name, facet_args = list(ncol = 2))
```

---

# Kernel density

```{r fig.height=5, fig.width=13, dev='svg'}
j <- c(1002:2001) # Discard first half as burn-in
mcmc_dens_overlay(x = sims[j,,], pars = parms_name)
```

---

# Pair plot

```{r fig.height=7, fig.width=13, dev='svg'}
mcmc_pairs(sims[j,,], pars = parms_name, off_diag_fun = "hex")
```

---

# Summary report

```{r}
monitor(sims, probs = c(0.025, 0.975) , digit=4) 
```

---

```{r, echo=F, warning=F, message=F}
library(tidyverse)
theme_set(theme_light())
X <- sims[j,,] %>% matrix(nrow = 1000*4) 
write.table(X, file = "setpts.out", row.names = F, sep = "\t")
X_setpts <- mcsim("linear.model.R", "linear_setpts.in.R")
```

```{r, echo=F}
vars <- names(X_setpts)
index <- which(vars == "y_1.1" | vars == "y_1.11")
X <- apply(X_setpts[index[1]:index[2]], 2, quantile,  c(0.5, 0.025, 0.975)) %>% t()
colnames(X) <- c("median", "LCL", "UCL")
df <- as.data.frame(X)
x <- seq(0, 10, 1)
df$x <- x
y <- c(0.0, 0.15, 2.32, 4.33, 4.61, 6.68, 7.89, 7.13, 7.27, 9.4, 10.0)
obs_data <- data.frame(x, y)
```

# Evaluation of model fit

```{r echo=F, fig.height=7, fig.width=12, dev='svg'}
ggplot(df, aes(x = x, y = median)) +
    geom_ribbon(aes(ymin = LCL, ymax = UCL), fill = "grey70", alpha = 0.5) + 
    geom_line() +
    geom_point(data = obs_data, x=x, y=y) +
    labs(x = "x", y = "y")
```

---

# Demo - linear model

## 1 Single chain testing

- **MCMC simulation** 

## 2 Multi-chains simulation

- **Check convergence**

- **Evaluation of model fit**

.footnote[
https://rpubs.com/Nanhung/NB_190523
]

---

# MCMC()

```r
# Input-file

MCMC();

# <Global assignments and specifications>

Level {

  Distrib();  
  Likelihood();

  # Up to 10 levels of hierarchy
  
  Simulation {
    # <Local assignments and specifications>
  }
  
  Simulation {
    # <Local assignments and specifications>
  }
  
  # Unlimited number of simulation specifications
} # end Level

End.
```

---
  
# MCMC()

The statement, gives general directives for MCMC simulations with following syntax:

```r
 MCMC("<OutputFilename>", "<RestartFilename>", "<DataFilename>",
          <nRuns>, <simTypeFlag>, <printFrequency>, <itersToPrint>,
          <RandomSeed>);
```

`"<OutputFilename>"` Output file name, the default is "MCMC.default.out"

`"<RestartFilename>"` Restart file name

`"<DataFilename>"` Data file name

`<nRuns>` an integer for the total sampling number (iteration)

`<simTypeFlag>` an integer (from 0 to 5) to define the simulation type

`<printFrequency>` an integer to set the interval of printed output 

`<itersToPrint>` an integer to set the number of printed output from the final iteration 

`<RandomSeed>` a numeric for pseudo-random number generator

---

# Simulation types

**`<simTypeFlag>` an integer (from 0 to 5) to define the simulation type**

`0`, start/restart a new or unfinished MCMC simulations

`1`, use the last MCMC iteration to quickly check the model fit to the data

`2`, improve computational speed when convergence is approximately obtained

`3`, tempering MCMC with whole posterior
 
`4`, tempering MCMC with only likelihood
 
`5`,  stochastic optimization 

---

# Check convergence

### Manipulate (MCSim under R)

`mcmc_array()`

### Visualize (**bayesplot**, **corrplot**)

`mcmc_trace()`  

`mcmc_dens_overlay()`  

`mcmc_pairs()`  

`corrplot()`

### Report (**rstan**)

`monitor()`  

---

# Parallel

![](https://i.ibb.co/b25QF2z/Screen-Shot-2019-05-20-at-3-08-17-PM.png)

---

# Demo - Ethylbenzene PBPK Model

### Model verification (tutorial 1)

**- Compare the simulated result with previous study**

### Uncertainty analysis (tutorial 2)

**- Set the probability distribution for model parameters**

### Morris elementary effects screening (tutorial 2)

**- Find the influential parameters**

### MCMC calibration (tutorial 3)

**- Estimate the "posterior"**

.footnote[
https://rpubs.com/Nanhung/NB_190523
]

---

background-image: url(https://i.ibb.co/q5vKKjJ/Screen-Shot-2019-05-13-at-4-43-31-PM.png)
background-size: 400px
background-position: 70% 30% 

# Bayesian Population Model

.large[**Individuals level**]

$E$: Exposure  

$t$: Time  

$\varphi$: measured parameters

$\theta$: unknown parameters  

$y$: condition the data

</br>

.large[**Population level**]

$\mu$: Population means

$\Sigma^2$: Population variances

$\sigma^2$: Residual errors

.pull-right[
.footnote[
https://doi.org/10.1007/s002040050284
]
]

---

# Population modeling

.code60[

.pull-left[

**Single level**

```r
# Comments of Input-file
<Global assignments and specifications>
  
  Level { # priors on population parameters
    
    Distrib ()
    Likelihood ()
    
    Level { # all subjects grouped
      
        Experiment { # 1 
          <Specifications and data in 1st simulation>
        } # end 1 
        Experiment { # 2
          <Specifications and data in 2nd simulation>
        } # end 2

      # ... put other experiments
      
    } # End grouped level  
  } # End population level
End. 
```    

]

.pull-right[

**Multi level**

```r
# Comments of Input-file
<Global assignments and specifications>
  
  Level { # priors on population parameters
    
    Distrib ()
    Likelihood ()
    
    Level { # individuals
      
      Distrib ()
      
      Level { # subject A
        Experiment { # 1 
          <Specifications and data in 1st simulation>
        } # end 1 
        Experiment { # 2
          <Specifications and data in 2nd simulation>
        } # end 2
      } # End subject A
      
      # ... put other subjects
      
    } # End individuals level  
  } # End population level
End. 
```    
]

]

---

# Demo - Tetrachloroethylene PBPK Model

## 1 Single level 

- Population-experiments

## 2 Multi level

- Population-individuals-experiments

.footnote[ https://rpubs.com/Nanhung/SM_190523 ]

---

# Final thoughts

MCMC simulation is a computational intensive process for metamodel or big data, one of the important issue today is how to improve the efficiency in the modeling.

- If possible, remove or turn-off unnecessary state variable (e.g., mass balance, metabolites). 

- Re-parameterization ([Chiu et al., 2006](https://rd.springer.com/article/10.1007/s00204-006-0061-9))

- Global sensitivity analysis and parameter fixing ([Hsieh et al., 2018](https://www.frontiersin.org/articles/10.3389/fphar.2018.00588/full))

- Parallel computing

- Grouped data

- Algorithm

- Software, hardware, ...

---

class: inverse, center, middle

# Thanks!

</br>

question? 


.footnote[
The slide, code, and record can find on my website: [nanhung.rbind.io](https://nanhung.rbind.io/talk/)
]

